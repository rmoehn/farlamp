\documentclass{farlamp}

% latexmk -pvc -pdfxe -bibtex -interaction=nonstopmode -outdir=build tiny-supfail.tex

\addbibresource{references.bib}

\subject{Report}
\title{Training a tiny SupAmp model on easy tasks}
\subtitle{The influence of failure rate on learning curves}
\author{Richard Möhn}
\date{\today}
\addtitledatatopdf

\begin{document}
\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

- To get a feeling for what is going on.
- And solve problems with the code with a short feedback loop. → Without having
to deal with big data, long training times and working in the cloud.
- Ran experiments with a tiny model on easy tasks.
- Still found results that are useful, because they open questions for further
research. See \OQ and \TODO markers.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}

- SupAmp on permutation powering.
- Tiny model. Means a one-layer transformer for both H' and X, if I understand
correctly.
- Permutation of four numbers, power up to seven.
- Train for 4000 steps, that is 4000 batches à 50 contexts with 4 questions each
to train X. However many steps the other threads got done in parallel.
- Then the same with $p_f \in {0.01, 0.1, 0.3, 1.0}$.
- Each of these three times.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Observations and discussion}

- Put these in one section, because this is an informal report and the
observations are handwaving in a way not suited to a pure observations section.

\subsection{Without errors}

- Low variance with a given configuration
- Learning curves with pf = 0 without surprises
- Train and validate remarkably close
- Smoothness of teacher learning curves


\subsection{With errors injected}

- Final accuracies and theoretical predictions
- Even pf = 0.01 lowers accuracy
- acc_on/2/teacher higher.
- What does it do differently?
(When it's short, put it in a misc section.)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Points to improve}

- performance
- length of training/checkpointing


\end{document}

Material collection
-------------------

- What hyperparameters?

- observations
    - low variance between learning curves for same hyperparameters → need to
    look at only one trial per configuration
    - all accuracies approach 1 quickly and without detours
    - behaviour makes sense/almost nothing surprising
        - first H' up to about 0.9, then on/2
        - first on/2 up, then on/3, /targets in between
    - train/validate are rather close

- The teacher learning curves look smoother than the others. This might be
because of Polyak averaging.

- observations with pf = 0

- expected final accuracies at pf = 0.01, 0.1, 0.3, 1.0
    - compared to actual final accuracies
- Why is acc_on/3 less than predicted theoretically?
    - Perhaps not trained to convergence.
- Even with pf = 0.01 the model doesn't get trained to overlook the errors.
    - Small evidence against the main hypothesis. – Would expect that the
    threshold is above 0.01, yet we do get the errors.
    - Might just be that the model has too great capacity/we're not regularizing
    enough.
    → Hypothesis: If I turn up regularization, the accuracies will rise. To a
    point where they don't fall exponentially in the question class/height
    anymore.
    - Of course, at some point the model can't be trained anymore. The error
    probability needs to be low enough that it can be countered with
    regularization weak enough as to not prevent learning.

- → X learns all the mistakes that H' makes

- The acc_on/2/teacher are higher than the acc_on/2/targets.
    - Can't say anything about acc_on/3/*, because training hadn't fully
    converged.
    - Is it because Polyak averaging acts as regularization?

- Training and validation curves are remarkably close.
    - Are the validation data not good enough? Too similar to the already
    trained data?
    - Maybe barely any overfitting, because there are always random new training
    data.

- performance conclusions see case 72
- need to train for longer/add checkpointing

- How do the model errors come about, though? The training data contain randomly
wrong inputs. What does the model learn to do differently when it receives
randomly wrong inputs?
