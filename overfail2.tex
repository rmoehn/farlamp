\documentclass{farlamp}

% latexmk -pvc -pdfxe -bibtex -interaction=nonstopmode -outdir=build overfail2.tex

\addbibresource{references.bib}

\subject{Analysis}
\title{Overseer failures in SupAmp and ReAmp}
\subtitle{How to determine their effect on overall failure rate}
\author{Richard Möhn\thanks{Work funded by Paul Christiano.}}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}

Amplifying an agent with low failure rate can cause the failure rate to grow
exponentially with the degree of amplification. To fix this, reliability
amplification can be used. Alternatively, we expect distillation by
reinforcement learning (RL) to prevent the failure rate from blowing up and even
increase reliability.
However, whether the increase will be sufficient, we don't know.
\parencite{ChriRelAmp} Reliability needs to be very high in high-stakes
situations \parencite[see][]{ChriLearCata}.

If RL alone is sufficient, we can avoid the computational and design overhead of
reliability amplification. Therefore, I will investigate how overseer failure
changes overall failure rate in the iterated distillation and amplification
scheme from \textcite{CSASupAmp}.

How to adapt this scheme to reinforcement learning is explored in \href{…}{…}.
In this preliminary analysis I lay out my current understanding of overseer
failure, related concepts and what I need to do. I will update it and answer
open questions on the way to implementation.


\section{Concepts}

Before I can write about my predictions, what experiments to run and how to
implement them, I will pin down some concepts.


\subsection{Factored cognition and factored evaluation}

- Ought terminology.
- In the following I will mostly write about the cognition case, because it is a
little more straightforward.
- It's easy to derive statements about the evaluation case from that.


\subsection{Overseer determinism}

The overseer might behave and fail deterministically or non-deterministically.
Ie. it might have a fixed output for each input, and thus the same failures for
a specific fraction of inputs. Or it might return a random value for any input,
with a certain probability.

I can't rule out either possibility. While I tend to deduce non-determinism from
the overseer model in \textcite{ChriRelAmp}, this deduction is countered by the
section on sequential decision making. Further, there are realistic examples of
both possibilities. If the overseer is a learned agent, as in
\textcite{CSASupAmp}, it is mostly deterministic. Although there, $H'$ is not
stationary, ie. its answers change over time. But I expect it to converge on
determinism. If the overseer is a human, as in Ought's factored evaluation
experiments \parencite{StuhDelCog}, it is non-deterministic. Of course, if the
human output is automated \parencite[see][sec. ‘Caching’ f.]{StuhTaxCapAmp},
determinism results.

% TODO: Put the main insights in a comment. This explains partly why above I
% tend to deduce. But I feel this has no space in this analysis.
appears to assume
non-determinism. The overseer is modelled as a distribution over policies, some
of which are adversarial. He doesn't specify whether to sample from this
distribution once before a whole training episode, or to resample at every step.
But the former is not realistic, because

% Why I tend to deduce non-determinism:
%
% If a sample from this distribution was drawn before a whole training episode,
% the whole training input might be adversarial. This appears less desirable
% than sampling a new overseer for each training step and therefore having
% adversity dispersed through the training. The latter corresponds to a
% non-deterministic overseer.
%
% Also sampling a "pure" overseer for a whole episode isn't possible. The result
% of the formalisation, the mixture, is realistic. The ingredients aren't.
% Because we wouldn't construct a completely adversarial policy. And we can't
% construct a completely benign one.
%
% Why sequential decision making counters: If the agents have to make a sequence
% of decisions, it sounds they're not resampled between decisions.
%
% Sometimes-adversarial deterministic policies are compatible with the
% "distribution A over policies", but not with the "view this as a simple
% mixture".


\subsection{Explicit and implicit tree}

In \textcite{CSASupAmp} the overseer answers questions using an implicit tree of
sub-questions. For example, suppose it is given a permutation $\sigma$ and has
to answer, ‘What is $\sigma^4(7)$?’. In order to answer this question, the
overseer asks two sub-questions to $X$: ‘What is $\sigma^2(7)$?’, and, ‘What is
$\sigma^2(\left<\text{answer to first sub-question}\right>)$?’
If $X$ is to answer correctly, it must have learned how to answer questions
about $\sigma^2$. Most likely, it got training data for that when the overseer
answered questions about $\sigma^2$ by asking two sub-questions each about
$\sigma$.

So there is a tree rooted in the question, ‘What is $\sigma^4(7)$?’ and whose
leaves are questions of the form, ‘What is $\sigma(n)$?’. This tree is never
explicit. $H$ decomposes only the root question and $X$ is expected to have
learned the answers to the sub-questions. Decomposing them recursively until it
reaches the primitive $\sigma(n)$ questions would be too expensive.

Analogous implicit tree structures arise for all tasks that SupAmp or ReAmp
solve. And the number of nodes in the tree grows exponentially with its height,
assuming that for each question $H$ asks at least two sub-questions.
Now suppose for a moment that the tree is explicit. Ie. the root question gets
decomposed recursively and each sub-question is answered by an agent. And
suppose that a wrong sub-answer anywhere in the tree entails a wrong root
answer. Then a wrong root answer is given with a probability that approaches 1
exponentially in the height of the tree \parencite{ChriRelAmp}.

This holds for the explicit tree. With the implicit tree a learning algorithm
stands between agent failure and overall failure. If $X$ is trained in a way
that it adopts $H$'s failures, the failure probability compounds as with an
explicit tree. If $X$ does not adopt $H$'s failures, because it recognizes them
as outliers, for example, the overall failure probability is equal to the
probability of $H$ answering the root question wrong.
How the training mechanism influences the overall failure probability is the
core question of this research project.


\subsection{Height of a question}

- The height of a question equals the height of the implicit tree of
sub-questions used to answer the question.
- A definition that doesn't depend on trees:

TODO: Make this just for sub-questions to give answer.
\begin{definition}{Height of a question} By recursion:
    \begin{enumerate}
    \item Primitive questions have height 0.
    \item A question has height $n$ if the maximum height of a sub-question
        asked by $H$ to generate a response is $n-1$.
    \end{enumerate}
The response will be an answer in the case of SupAmp and a reward in the case of
ReAmp.
\end{definition}

\begin{example}
    The decomposition rule for \task{permutation powering} from \textcite[table
    3]{CSASupAmp} is:
    \begin{align}
        \sigma^{2k}(x) &= \sigma^k(\sigma^k(x)) \\
        \sigma^{2k+1}(x) &= \sigma(\sigma^k(\sigma^k(x)))
    \end{align}

    \begin{itemize}
        \item ‘What is $\sigma(4)$?’, has height 0, because it is a primitive
            question.
        \item ‘What is $\sigma^2(4)$?’, has height 1, because $H$ decomposes it
            into ‘What is $\sigma(4)$?’ and ‘What is $\sigma(\left<\text{answer
            to the first sub-question}\right>)$, which are primitive questions
            (height 0).
        \item ‘What is $\sigma^3(4)$?’, has height 1.
        \item ‘What is $\sigma^4(4)$?’, has height 2.
        \item ‘What is $\sigma^8(4)$?’, has height 3.
    \end{itemize}
\end{example}


\section{Prediction}
\label{sec:prediction}

- my expectation (is it already in ChriRelAmp?): once the failure probability
gets greater than a threshold, the overall failure probability goes up
- Threshold depends on the parameters like determinism and learning
mechanism
- Tried to make further predictions, but so far can't write down good
reasons, because I know too little about the details of SL, RL.
- For example, \textcite{ChriRelAmp} predicts: ‘if the overseer fails with
probability $1 \%$, then this only changes the reward function by 0.01, and an
RL agent should still avoid highly undesirable actions’
- Can't the same be said for SL? – Only $1 \%$ of samples will be wrong and
assuming enough regularization to make the learner learn small concepts, it
would treat these samples as outliers.
- Note that \textcite{ChriRelAmp} doesn't make a statement about SL. He only
predicts that IL won't be robust. I imagine IL to be closer to a lookup table
than to a concept learner, so this makes sense. Although we couldn't solve the
tasks from \textcite{CSASupAmp} with a lookup table, because they are
combinatorial and need the learner to learn concepts.

- Also expect that a non-deterministic agent will mean a higher threshold than a
non-deterministic one, because it doesn't hammer home some wrong input.
- On the other hand, if the domain of input is large, wrong input will only have
a small chance of being hammered.
- Another question where I can hardly make a prediction.


\section{Experiments}

- compare SL and RL with different failure rates and perhaps
non-deterministic vs. deterministic overseer
- perhaps different degrees of regularization, too
- how to model the failure?
    - overseer outputs random answer (SL) or reward (RL) with certain
    probability
    - deterministic failure: same output for same input
    - non-deterministic failure: various outputs for various inputs
    - Here it appears better to inject the failure in the output of $H'$ rather
    than making $H$ fail. Because the latter would just lower the learning
    performance of $H'$/there would be a layer between where the failure happens
    and the place where we interested in observing the effects.
    - So do the same for deterministic.

\printbibliography
\end{document}
