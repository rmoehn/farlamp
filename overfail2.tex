\documentclass{farlamp}

% latexmk -pvc -pdfxe -bibtex -interaction=nonstopmode -outdir=build overfail2.tex

\addbibresource{references.bib}

\subject{Analysis}
\title{Overseer failures in SupAmp and ReAmp}
\subtitle{How to determine their effect on overall failure rate}
\author{Richard Möhn\thanks{Work funded by Paul Christiano.}}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}

(For a general overview and a glossary, see the
\href{https://github.com/rmoehn/farlamp}{home page of the Farlamp
repository.})

Capability amplification can cause the failure rate of an ML system to blow up,
even when the amplified agent fails rarely. To fix this, reliability
amplification can be used. Alternatively, we expect distillation by
reinforcement learning (RL) to keep the failure rate down and even decrease it
below the agent's. However, whether the result will be reliable enough, we don't
know.\parencite{ChriRelAmp} But we do need to know in high-stakes situations
\parencite[see][]{ChriLearnCata}.

If RL alone is sufficient, we can avoid the computational and design overhead of
reliability amplification. Therefore, I will investigate how overseer failure
changes the overall failure rate, using the iterated distillation and
amplification scheme from \textcite{CSASupAmp}.

How to adapt this scheme to reinforcement learning is analysed in
\href{https://github.com/rmoehn/farlamp/raw/master/supamp-reamp.pdf}{How to turn
SupAmp into ReAmp?}. In this preliminary analysis I lay out my current
understanding of overseer failure, related concepts and what I need to do. I
will update it and answer open questions on the way to implementation.


\section{Concepts}

Before I can write about my predictions, what experiments to run and how to
implement them, I will pin down some terms (bold) and concepts.


% Why am I writing all this? Because it hasn't been laid out clearly before.
% Paul introduces the problem alright, but he assumes a lot of knowledge about
% IDA and why he's talking about amplification at one point and adds in learning
% algorithms at another.

\subsection{General terms}

In the following, being more \Term{capable} means being able to answer harder
questions. The \Term{overseer} is a human or some extract of human judgment. The
\Term{assistant} is a machine learning (sub-)system. The assistant can answer
the overseer's questions. And the overseer can give data to the algorithm
that trains the assistant to become more capable.


\subsection{Iterated amplification and distillation}

The overseer alone can only provide training data until the assistant's
capability roughly equals the overseer's. After this the training can be
continued by using \Term{Iterated distillation and amplification} (IDA).
\Term{Distillation} is the mechanism by which the the assistant is trained.
Amplfication refers to the assistant helping the overseer generate training data
by answering questions.

IDA has been explained in other places \parencite{CotrIDA, ESSMLPIDA}. But in
order to write this analysis clearly I need the notion of amplification without
immediate distillation. For this I don't know of any brief explanation.

Suppose you want to train an assistant $X$ \parencite[symbols taken
from][]{CSASupAmp} to give a solution to problem $p$. If you want to train $X$
using supervised learning (SL), you ask yourself: ‘What is the solution to $p$?’
You answer: ‘The solution is $s$.’ $(p, s)$ is the training datum for $X$.

If you want to train $X$ using reinforcement learning (RL), $X$ has to suggest a
solution $s$, then you ask yourself: ‘What reward should I give $X$ for solution
$s$ to problem $p$?’ You answer: ‘The reward should be $r$’. $r$ is the training
datum for $X$.

After some time $X$ can solve all the problems that you can solve. But you want
it to become able solve harder problems, for which you alone can't answer the
question and therefore can't provide training data. What do you do? You break
the question down to a tree of sub-questions. The tree grows downwards by adding
sub-questions. The further we go towards the leaves, the easier the questions
become. The leaf questions are so easy that we can answer them without breaking
them down further. Then the answers flow upwards and are combined at each node
until we obtain the root answer. Root answer (and root question in the case of
SL) can be used as training datum for $X$. This is \Term{iterated
amplification} (IA), without distillation. (For examples of trees, see
\textcite{StuhFacCog}.)

This breaking down of questions is computationally expensive, though. What if
you had an assistant $X^{-1}$ that is almost as capable as $X$? Then you could
break down only the top-level question into sub-questions, have them answered by
$X^{-1}$, and use these sub-answers to answer the root question. Again you've
obtained a training datum for $X$.

This is the first iteration of iterated \Term{distillation} and amplification.
-- You train an agent $X^0$ until it is roughly your level. The combination of
you and the agent forms a more capable compound agent $HX^0$. Then $HX^0$
generates data to train the next agent $X^1$, which is more capable than $X^0$.
This is called ‘distilling’ $HX^0$. Now you can team up with $X^1$ to become
$HX^1$ and generate data to train $X^2$. And so on.

Now we add failure. It's natural that you sometimes make a mistake and return a
wrong answer. Suppose you're doing IA. And a wrong sub-answer anywhere in the
tree leads to the root answer being wrong. And each non-leaf question leads to
at least two sub-questions. Then even if your failure probability is very small,
the probability of ending up with a wrong root answer approaches 1 exponentially
quickly in the height of the tree.\parencite{ChriRelAmp}

What happens with I\emph{D}A isn't so clear. It depends on how the $X^n$ are
trained. If the training algorithm causes them to faithfully copy all your
answers, the failure probability will compound as in IA. But if the training
algorithm makes the $X^n$ good at ‘recognizing concepts’, they will treat some
wrong inputs as outliers and the failure probability won't compound as quickly,
or even go down as $n$ grows. Of course, the more mistakes you make, the harder
it becomes for a learner to recognize right from wrong. The goal of the Farlamp
project is to find out how the training algorithm influences the failure rate.


\subsection{Overseer determinism and non-determinism}

The overseer might behave and fail deterministically or non-deterministically.
Ie. it might have a fixed output for each input, and thus the same failures for
a specific fraction of inputs. Or it might return a random value for any input,
with a certain probability.

I can't rule out either possibility. While I tend to deduce non-determinism from
the overseer model in \textcite{ChriRelAmp}, this deduction is countered by the
section on sequential decision making. Further, there are realistic examples of
both possibilities. If the overseer is a learned agent, as in
\textcite{CSASupAmp}, it is mostly deterministic. Although there, $H'$ is not
stationary, ie. its answers change over time. But I expect it to converge on
determinism. If the overseer is a human, as in Ought's factored evaluation
experiments \parencite{StuhDelCog}, it is non-deterministic. Of course, if the
human output is automated \parencite[see][sec. ‘Caching’ f.]{StuhTaxCapAmp},
determinism results.

% Why I tend to deduce non-determinism:
%
% If a sample from this distribution was drawn before a whole training episode,
% the whole training input might be adversarial. This appears less desirable
% than sampling a new overseer for each training step and therefore having
% adversity dispersed through the training. The latter corresponds to a
% non-deterministic overseer.
%
% Also sampling a "pure" overseer for a whole episode isn't possible. The result
% of the formalisation, the mixture, is realistic. The ingredients aren't.
% Because we wouldn't construct a completely adversarial policy. And we can't
% construct a completely benign one.
%
% Why sequential decision making counters: If the agents have to make a sequence
% of decisions, it sounds they're not resampled between decisions.
%
% Sometimes-adversarial deterministic policies are compatible with the
% "distribution A over policies", but not with the "view this as a simple
% mixture".


\subsection{Height of a question}

A question is answered using an implicit tree of sub-questions and the height of
that tree is the height of the question. In addition, here is a definition that
doesn't depend on trees.

\begin{definition}[Height of a question] By recursion:
    \begin{enumerate}
    \item Primitive questions have height 0.
    \item A question has height $n$ if $n-1$ is the maximum height of a
        sub-question asked by $H$ to generate a response.
    \end{enumerate}
The response will be an answer in the case of SupAmp and a reward in the case of
ReAmp.
\end{definition}

\begin{example}
    The decomposition rule for \task{permutation powering} from \textcite[table
    3]{CSASupAmp} is:
    \begin{align}
        \sigma^{2k}(x) &= \sigma^k(\sigma^k(x)) \\
        \sigma^{2k+1}(x) &= \sigma(\sigma^k(\sigma^k(x)))
    \end{align}

    \begin{itemize}
        \item ‘What is $\sigma(4)$?’, has height 0, because it is a primitive
            question.
        \item ‘What is $\sigma^2(4)$?’, has height 1, because $H$ decomposes it
            into ‘What is $\sigma(4)$?’ and ‘What is $\sigma(\left<\text{answer
            to the first sub-question}\right>)$, which are primitive questions
            (height 0).
        \item ‘What is $\sigma^3(4)$?’, has height 1.
        \item ‘What is $\sigma^4(4)$?’, has height 2.
        \item ‘What is $\sigma^8(4)$?’, has height 3.
    \end{itemize}
\end{example}


\section{Predictions}
\label{sec:prediction}

I expect that there is a threshold. If the overseer's failure probability is
above that threshold, IDA increases the overall failure probability. If the
overseer's failure probability is below the threshold, IDA decreases the overall
failure probability. \textcite{ChriRelAmp} might have expressed this before, but
I don't quite understand the relevant sections.

And I expect that the threshold depends on a few parameters, such as whether $H$
is deterministic and how $X$ is trained. I tried to predict in which direction
each parameter pushes the threshold, but so far my ideas have led me into the
fog, because I know too little about the details of SL and RL.

For example, \textcite{ChriRelAmp} reasons: ‘if the overseer fails with
probability $1 \%$, then this only changes the reward function by 0.01, and an
RL agent should still avoid highly undesirable actions’ Can't the same be said
for SL? If only $1 \%$ of the training data for $X$ are wrong, won't it treat
them as outliers, provided enough regularization?

Note that \textcite{ChriRelAmp} doesn't state anything about SL. He only
predicts that distillation by IL will decrease reliability. I imagine IL closer
to a lookup table than to a concept learner, so it makes sense that it will
replicate $H$'s failures. But a lookup table wouldn't hold the amount of
possible of the tasks in \textcite{CSASupAmp} anyway.

I do think that non-determinism will result in a greater failure blow-up
threshold, because a randomly wrong label will be overridden later by the
correct one. Conversely a overseer would repeatedly give the learner the same
wrong label. One might argue such repetitions are rare if the domain is large.
But the domains start out fairly small in the training schedule of
\textcite{CSASupAmp}. And the start is where the foundation for the rest of the
training is laid. If this foundation is faulty, correct results cannot be built.


\section{Experiments}

I want to measure the overall failure rate of SupAmp and ReAmp, given various
overseer failure rates, a non-deterministic and a deterministic overseer, and
perhaps different degrees of regularization. I will decide the details later.

In the non-deterministic case overseer failure can be modelled by injecting a
random answer (SupAmp) or random evaluation (ReAmp) into the output of $H'$ with
a certain probability. One might think that $H$ should fail, because it is the
actual overseer and $H'$ its imitation. But that would just degrade the learning
performance of $H'$ and not deliver randomly wrong training inputs to $X$ as
required.

The deterministic case is similar, except that a small fixed set of inputs must
be mapped to fixed random outputs. This can be achieved with a lookup table or
by hashing the input to provide a fail/no fail value and in the former case
hashing them again to get the random-but-fixed output.


\printbibliography
\end{document}
