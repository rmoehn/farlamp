\documentclass{farlamp}

% latexmk -pvc -pdfxe -bibtex -interaction=nonstopmode -outdir=build overfail2.tex

\addbibresource{references.bib}

\subject{Analysis}
\title{Overseer failures in SupAmp and ReAmp}
\subtitle{How to determine their effect on overall failure rate}
\author{Richard Möhn\thanks{Work funded by Paul Christiano.}}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}

Amplifying an agent with low failure rate can cause the failure rate to grow
exponentially with the degree of amplification. To fix this, reliability
amplification can be used. Alternatively, we expect distillation by
reinforcement learning (RL) to prevent the failure rate from blowing up and even
increase reliability.
However, whether the increase will be sufficient, we don't know.
\parencite{ChriRelAmp} Reliability needs to be very high in high-stakes
situations \parencite[see][]{ChriLearnCata}.

If RL alone is sufficient, we can avoid the computational and design overhead of
reliability amplification. Therefore, I will investigate how overseer failure
changes overall failure rate in the iterated distillation and amplification
scheme from \textcite{CSASupAmp}.

How to adapt this scheme to reinforcement learning is explored in
\href{https://github.com/rmoehn/farlamp/raw/master/supamp-reamp.pdf}{How to turn
SupAmp into ReAmp?}. In this preliminary analysis I lay out my current
understanding of overseer failure, related concepts and what I need to do. I
will update it and answer open questions on the way to implementation.


\section{Concepts}

Before I can write about my predictions, what experiments to run and how to
implement them, I will pin down some terms and concepts.


\subsection{Iterated distillation an amplification}

% Why am I writing all this? Because it hasn't been laid out clearly before.
% Paul introduces the problem alright, but he assumes a lot of knowledge about
% IDA and why he's talking about amplification at one point and adds in learning
% algorithms at another.

- Being more capable corresponds to being able to answer harder questions.
- Will talk about an overseer and an assistant.
- The overseer is a human or some analog to human judgment.
- The assistant is a machine learning (sub-)system, which the overseer can train to
answer questions.
- The overseer alone can only provide training input until the assistant's
capability roughly equals that of the overseer.
- Iterated distillation and amplification is a scheme for an overseer to
continue providing training input for the assistant after it becomes as capable as
the overseer.

- Distillation is the part in which the agent is trained.
- Amplification is the part in which the overseer is assistend by the agent.

% - If the overseer can ask sub-questions to a capable agent, it can
% answer harder questions than it can without assistance. Thus it is able to train
% a more capable agent than itself.
% - The more capable the assistant, the harder questions the overseer can answer,
% the higher-level agent it can train.

% - How do you answer a difficult question? By dividing it into sub-questions. If
% the sub-questions are difficult, recur to the first step. If a sub-question is
% easy, answer it. When all sub-answers for a difficult question are given, use
% them to answer the difficult question. Until all difficult questions, and
% therefore also the question at the root are answered.
% - This results in a tree of questions.
% - This is iterated amplification.

% - Assume more knowledge?
% - The output of the tree is training data for the agent?

% - Suppose you want to answer a question and the answer to the question will
% serve as training data to train agent $X$.
% - $X$ might be trained using supervised learning. In this case the training data
% will be the (question, answer) pair itself. Or it might be trained using $X$
% reinforcement learning, in which case the question is, ‘

- IDA has been explained in other places \parencite{CotrIDA, ESSMLPIDA}.
- In order to write clearly I need the notion of a tree and amplification
without distillation. This doesn't exist in other places.

- Suppose you want to train an assistant $X$ to give a solution to problem $p$.
- If you want to train $X$ using supervised learning (SL), you ask yourself:
‘What is the solution to $p$?’ You answer: ‘The solution is $s$.’ $(p, s)$
is the training datum for $X$.
- If you want to train $X$ using reinforcement learning (RL), $X$ has to suggest
a solution $s$, then you ask yourself: ‘What reward should I give $X$ for
solution $s$ to problem $p$?’ You answer: ‘The reward should be $r$’. $r$
is the training datum for $X$.
- So you, the overseer, are always given a question and you must return an
answer.

- There comes a point when $X$ can solve all the problems that you can solve.
But you want it to solve harder problems, for which you can't answer the
question. What do you do? You break the question down into a tree of
sub-questions. (For examples of trees, see \textcite{StuhFacCog}.) Every time
you hit a hard question, you branch out. This tree grows by adding sub-questions
and from these branches sub-answers flow back up. The further we go towards the
leaves, the easier the questions get and the leaf questions are so easy that we
can answer them without breaking them down further.
- This is iterated amplification.

- It's tedious to do all this breaking down though. What if you had an
assistant $X^{-1}$ that is almost as capable as $X$? Then you could just break down the
top-level question into sub-questions, feed them to $X^{-1}$, use the
sub-answers to answer the top-level question and the answer to that to train
$X$.
- This is how iterated distillation and amplification works.
- You train an agent $X^0$ until it is roughly your level. The combination of
you and the agent forms a more capable compound agent $HX^0$. Then $HX^0$
generates data to train the next agent $X^1$ that is more capable than $X^0$.
This is called ‘distilling’ $HX^0$. Now you can team up with $X^1$ to become
$HX^1$ and generate data to train $X^2$.

- Now we throw failure into the mix. Suppose you make a mistake and return a
wrong answer, with a small probability. If you did iterated amplification, and
any wrong sub-answer leads to the top-level answer being wrong, then the
probability of ending up with a wrong top-level answer grows quickly the higher
the tree.

- With iterated \emph{distillation} and amplification this isn't a sure thing.
- If the training algorithm causes the $X^n$ to faithfully copy every
answer, the same compounding of the failure probability will result.
- But if the training algorithm makes the $X^n$ good at ‘recognizing concepts’,
they will treat wrong inputs as outliers and the failure probability won't
compound as quickly, or even go down as $n$ grows.
- Of course, the more often mistakes you make, the harder it becomes for a
learner to recognize right from wrong.
- [In fact, when the problems are complex, you can't provide every possible
problem and solution to $X^n$. So it needs to recognize concepts.]
- The goal of the Farlamp project is to find out how the training algorithm
influences the failure rate.

TODO: Make sure that this terminology is consistent with Pauls other work and
CotrIDA.

% - Too much! I have to start with assuming an understanding of IDA. Nah, someone
% who doesn't know it would have to read the whole of Factored Cognition instead
% of three paragraphs here.

% - Iterated distillation and amplification (IDA) is a scheme for an overseer to
% continue providing training input for the agent after it becomes as capable as
% the overseer. If you don't know how it works, read \textcite{CotrIDA} before you
% continue here.
% - Iterated amplification is similar, except that the overseer doesn't train
% assistants. It just makes copies of itself

% - Why is this so damn hard to describe clearly?


\subsection{Providing and evaluating answers}

In SupAmp \parencite{CSASupAmp} the overseer provides answers to root questions.
If we want to replace the distillation by SL with distillation by RL, the
overseer needs to evaluate answers that $X$ proposes. In both cases does the
overseer ask sub-questions and it uses the sub-answers to determine its reply.
Ought calls these two modes of IDA \parencite[see][]{CotrIDA} ‘factored
cognition’ \parencite{StuhFacCog} and ‘factored evaluation’
\parencite{StuhDelCog}.

I will mostly write about the SL case, because it is slightly more
straightforward. You can easily derive analogous statements about the RL case.


\subsection{Overseer determinism and non-determinism}

The overseer might behave and fail deterministically or non-deterministically.
Ie. it might have a fixed output for each input, and thus the same failures for
a specific fraction of inputs. Or it might return a random value for any input,
with a certain probability.

I can't rule out either possibility. While I tend to deduce non-determinism from
the overseer model in \textcite{ChriRelAmp}, this deduction is countered by the
section on sequential decision making. Further, there are realistic examples of
both possibilities. If the overseer is a learned agent, as in
\textcite{CSASupAmp}, it is mostly deterministic. Although there, $H'$ is not
stationary, ie. its answers change over time. But I expect it to converge on
determinism. If the overseer is a human, as in Ought's factored evaluation
experiments \parencite{StuhDelCog}, it is non-deterministic. Of course, if the
human output is automated \parencite[see][sec. ‘Caching’ f.]{StuhTaxCapAmp},
determinism results.

% Why I tend to deduce non-determinism:
%
% If a sample from this distribution was drawn before a whole training episode,
% the whole training input might be adversarial. This appears less desirable
% than sampling a new overseer for each training step and therefore having
% adversity dispersed through the training. The latter corresponds to a
% non-deterministic overseer.
%
% Also sampling a "pure" overseer for a whole episode isn't possible. The result
% of the formalisation, the mixture, is realistic. The ingredients aren't.
% Because we wouldn't construct a completely adversarial policy. And we can't
% construct a completely benign one.
%
% Why sequential decision making counters: If the agents have to make a sequence
% of decisions, it sounds they're not resampled between decisions.
%
% Sometimes-adversarial deterministic policies are compatible with the
% "distribution A over policies", but not with the "view this as a simple
% mixture".


\subsection{Explicit and implicit tree}

In \textcite{CSASupAmp} the overseer answers questions using an implicit tree of
sub-questions. For example, suppose it is given a permutation $\sigma$ and has
to answer, ‘What is $\sigma^4(7)$?’. In order to answer this question, the
overseer asks two sub-questions to $X$: ‘What is $\sigma^2(7)$?’, and, ‘What is
$\sigma^2(\left<\text{answer to first sub-question}\right>)$?’
If $X$ is to answer correctly, it must have learned how to answer questions
about $\sigma^2$. Most likely, it got training data for that when the overseer
answered questions about $\sigma^2$ by asking two sub-questions each about
$\sigma$.

So there is a tree rooted in the question, ‘What is $\sigma^4(7)$?’ and whose
leaves are questions of the form, ‘What is $\sigma(n)$?’. This tree is never
explicit. $H$ decomposes only the root question and $X$ is expected to have
learned the answers to the sub-questions. Decomposing them recursively until it
reaches the primitive $\sigma(n)$ questions would be too expensive.

Analogous implicit tree structures arise for all tasks that SupAmp or ReAmp
solve. And the number of nodes in the tree grows exponentially with its height,
assuming that for each question $H$ asks at least two sub-questions.
Now suppose for a moment that the tree is explicit. Ie. the root question gets
decomposed recursively and each sub-question is answered by an agent. And
suppose that a wrong sub-answer anywhere in the tree entails a wrong root
answer. Then a wrong root answer is given with a probability that approaches 1
exponentially in the height of the tree \parencite{ChriRelAmp}.

This holds for the explicit tree. With the implicit tree a learning algorithm
stands between agent failure and overall failure. If $X$ is trained in a way
that it adopts $H$'s failures, the failure probability compounds as with an
explicit tree. If $X$ does not adopt $H$'s failures, because it recognizes them
as outliers, for example, the overall failure probability is equal to the
probability of $H$ answering the root question wrong.
How the training mechanism influences the overall failure probability is the
core question of this research project.


\subsection{Height of a question}

A question is answered using an implicit tree of sub-questions and the height of
that tree is the height of the question. In addition, here is a definition that
doesn't depend on trees.

\begin{definition}[Height of a question] By recursion:
    \begin{enumerate}
    \item Primitive questions have height 0.
    \item A question has height $n$ if $n-1$ is the maximum height of a
        sub-question asked by $H$ to generate a response.
    \end{enumerate}
The response will be an answer in the case of SupAmp and a reward in the case of
ReAmp.
\end{definition}

\begin{example}
    The decomposition rule for \task{permutation powering} from \textcite[table
    3]{CSASupAmp} is:
    \begin{align}
        \sigma^{2k}(x) &= \sigma^k(\sigma^k(x)) \\
        \sigma^{2k+1}(x) &= \sigma(\sigma^k(\sigma^k(x)))
    \end{align}

    \begin{itemize}
        \item ‘What is $\sigma(4)$?’, has height 0, because it is a primitive
            question.
        \item ‘What is $\sigma^2(4)$?’, has height 1, because $H$ decomposes it
            into ‘What is $\sigma(4)$?’ and ‘What is $\sigma(\left<\text{answer
            to the first sub-question}\right>)$, which are primitive questions
            (height 0).
        \item ‘What is $\sigma^3(4)$?’, has height 1.
        \item ‘What is $\sigma^4(4)$?’, has height 2.
        \item ‘What is $\sigma^8(4)$?’, has height 3.
    \end{itemize}
\end{example}


\section{Predictions}
\label{sec:prediction}

I expect that there is a threshold. If the overseer's failure probability is
above that threshold, IDA increases the overall failure probability. If the
overseer's failure probability is below the threshold, IDA decreases the overall
failure probability. \textcite{ChriRelAmp} might have expressed this before, but
I don't quite understand the relevant sections.

And I expect that the threshold depends on a few parameters, such as whether $H$
is deterministic and how $X$ is trained. I tried to predict in which direction
each parameter pushes the threshold, but so far my ideas have led me into the
fog, because I know too little about the details of SL and RL.

For example, \textcite{ChriRelAmp} reasons: ‘if the overseer fails with
probability $1 \%$, then this only changes the reward function by 0.01, and an
RL agent should still avoid highly undesirable actions’ Can't the same be said
for SL? If only $1 \%$ of the training data for $X$ are wrong, won't it treat
them as outliers, provided enough regularization?

Note that \textcite{ChriRelAmp} doesn't state anything about SL. He only
predicts that distillation by IL will decrease reliability. I imagine IL closer
to a lookup table than to a concept learner, so it makes sense that it will
replicate $H$'s failures. But a lookup table wouldn't hold the amount of
possible of the tasks in \textcite{CSASupAmp} anyway.

I do think that non-determinism will result in a greater failure blow-up
threshold, because a randomly wrong label will be overridden later by the
correct one. Conversely a overseer would repeatedly give the learner the same
wrong label. One might argue such repetitions are rare if the domain is large.
But the domains start out fairly small in the training schedule of
\textcite{CSASupAmp}. And the start is where the foundation for the rest of the
training is laid. If this foundation is faulty, correct results cannot be built.


\section{Experiments}

I want to measure the overall failure rate of SupAmp and ReAmp, given various
overseer failure rates, a non-deterministic and a deterministic overseer, and
perhaps different degrees of regularization. I will decide the details later.

In the non-deterministic case overseer failure can be modelled by injecting a
random answer (SupAmp) or random evaluation (ReAmp) into the output of $H'$ with
a certain probability. One might think that $H$ should fail, because it is the
actual overseer and $H'$ its imitation. But that would just degrade the learning
performance of $H'$ and not deliver randomly wrong training inputs to $X$ as
required.

The deterministic case is similar, except that a small fixed set of inputs must
be mapped to fixed random outputs. This can be achieved with a lookup table or
by hashing the input to provide a fail/no fail value and in the former case
hashing them again to get the random-but-fixed output.


\printbibliography
\end{document}
